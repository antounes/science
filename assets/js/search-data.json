{
  
    
        "post0": {
            "title": "Classifying Rock Types with Deep Learning: A Trial",
            "content": "Introduction . This personal research work should help me getting a sound understanding of how convolutional neural networks work under the hood. Notebooks help mixing computer science practice and deep learning theory. . The goal here is to build a classifier for distinguishing between rock types. Part of my studies focused on Eart Science, and before definitely switching to math and computer science, I spent a decent part of my time as a field geologist student. One of the most tricky parts of the job was identifying rocks, be it directly on the field or after, when back in the lab. . The image below shows the three major families of rocks geologists distinguish, namely igneous (formed through cooling and solidification of lava), metamorphic (formed by transformation of existing rocks under varying pressure and temperature conditions) and sedimentary (formed by accumulation or deposition of mineral or organic particles). . . Let&#39;s assess if such rocks are understandable and can be represented in such a manner that a neural network can distinguish between their types. . The Dataset . Our rocks dataset has been prepared by people from the Brac University Mars Rover Team for the purpose of research of similar rocks identification in a mars-like surface and is freely available on Kaggle. . The exercice here consists in classifying rocks not only as igneous, sedimentary or metamorphic, but as rock names. Our dataset includes the following rocks for each rock family: . Igneous: basalt, granite | Metamorphic: marble, quartzite | Sedimentary: coal, limestone, sandstone | . 2069 rock images copied to rocks 0 corrupted file(s) . Preparing Data for Training The Network . Creating Suitable Data Structures . Convolutional neural networks accept images as inputs. They won&#39;t accept jpg, png, or any other image file format though. Generally speaking, when developing deep learning models, our first thoughts must concern data representation, be it technically or conceptually. . Technically, models accept numbers. Deal with it. We&#39;ll have to find a way of passing our downloaded images to the model we&#39;ll build in a way the model will be able to process them and produce outputs from them. Conceptually, before diving headfirst in code, carefully thinking of a way of representing raw data can save hours and prevent headaches that could arise when dealing with neural network architecture considerations. . We must also define how to create the sets used for training and evaluating our model. Here, the variable to predict (the dependent variable) is a rock name $-$ basalt, granite, marble, quartzite, coal, limestone or sandstone. The inputs used by the model for outputing classifications (the independent variables) are rock images. Building the training and validation sets must be carried out carefully, and significant performance improvements can come only from a wise selection of training and validation examples for feeding a network. Here we&#39;ll perform random splitting of the images, with a fraction of 20% being used as validation set. . Images downloaded are of different sizes. We can have a look at the size distribution below (for the sake of clarity, only sizes counting more than 10 images have been plotted here). This is a problem since we won&#39;t feed our network one image at a time, but rather batches of images (technically we&#39;re feeding mini-batches, but this will be discussed later). Before grouping it into a structure that can be sliced into such mini-batches, we need to resize the images to the same size. . When all these preparation steps have been defined and carried out, we&#39;re ready to build our data structures and provide them to the network. We can look at a few images from the training and validation batches we just created. . Let&#39;s pause a second for an important word here: resizing images will somewhat modify the reality. The model will be fed with images that may be cropped, stretched, etc. . A very crucial point in machine learning (and consequently in deep learning) is that a models cannot perform on concepts they haven&#39;t been trained on. Feeding a neural network with black and white images and then trying to produce outputs on never-seen coloured images can only result in poor performance. The goal of deep learning is to find useful representations of data, be it in a humanly understandable space or not. Models cannot invent representations while they haven&#39;t been trained on, they&#39;re just searching for better and more explainable representations of the input data. That&#39;s why data preparation is maybe the most important point in building deep learning models. . Data Augmentation . When talking about supervised learning, one can often hear that machine learning needs loads of labeled data for giving satisfying results, and so does deep learning. Although this is not totally wrong, sometimes you cannot just gather more than the data you&#39;ve already passed to your model for training. . Data augmentation is a way of getting more data without actually having to gather and label more data. In this context, we&#39;ll use data augmentation to provide our model with more varieties of the images we have in our training set. Concretely, images can be rotated, flipped, stretched, their contrast and brightness can be changed, etc. What&#39;s of critical importance is to bear in mind that augmentations must be applied in such a way that the reality of our data (let&#39;s just think about this concept as the underlying generative process from which our data at hand emerged) is not changed. Below is an example of augmentation applied to a sample of our training set. . A word on data augmentation. Data augmentation is part of the more general process of data preparation, a step that consists in cleaning the data and getting it ready for training and evaluating a model. As we said, the meaning of the data must not be changed, so transformations applied during this step should be carefully chosen, and this decision falls to data scientists. Rules on how transformation must be concretely applied to the data can lead to discussion. Some people agree with the fact that data augmentation must be applied only to training data, while other argue it can be applied to training and test data. More on this topic can be found on popular discussion forums such as cross-validated. . Training the model . The model we&#39;ll use for this example is a convolutional neural network (CNN). As of today, CNN models represent the state of the art approach to computer vision problems. The CNN architecture used here is ResNet18, a residual network with 18 layers. . Since this introductory blog post only focuses on surface theory, we&#39;ll stick to simple ways of watching at our model performance on this problem. Later post will dig deeper and develop some concepts around performance metrics, overfitting and generalisation, etc. . The visualisation showed below is pretty straightforward to interpret: rows represent the actual labels assigned to our validation examples, while columns represent values output by the model for these examples. As you can see by the colour-code, darker cells encode higher numbers. . This object is called a confusion matrix. Such object is very useful when computing model performance metrics and transcribing it into simple and easy-to-understand numbers. As we can see, our model performed quite well on coal (95% of the samples actually being coal correctly classified as coal), sandstone (88% of the samples actually being a sandstone classified as sandstones) and limestone (83% of the samples actually being a limestone classified as limestones). It behaved not that bad on quartzite and marble (73% and 65% of correct classifications respectively), but wasn&#39;t performant enough on recognising granites and basalt (48% and 44% of correctly classified samples respectively). . Conclusion . This pretty basic project showed how to quickly get results with a classification model, while navigating through some concepts of applying deep learning to computer vision problems. Theory was quite superficially presented here, and important points to bear in mind would concern need for data quality, correct construction of training and validation sets, and evaluation of model performance. . Our model did not perform very well at recognising all the rock types in our dataset. Surprisingly, errors magnitudes were quite grouped by rock families, with lower errors related to sedimentary rocks classification (coal, limestone and sandstone) and higher errors related to igneous rocks (basalt and granite). These errors might be due to several aspects: data could be wrongly labelled (actual limestones could be labelled as sandstones), our model could be badly built, or lacking optimisation steps. . This simple project shows evidence of the fundamental engineering science aspect of machine learning (and here deep learning). Every machine learning project consists in rigourously going back and forth between trying, assessing performance, tuning, and trying again. At the end of the day, what matters is trying and failing while continuously iterating towards our objective. .",
            "url": "https://antounes.github.io/science/2021/12/30/classyfing-rocks-with-deep-learning-a-trial.html",
            "relUrl": "/2021/12/30/classyfing-rocks-with-deep-learning-a-trial.html",
            "date": " • Dec 30, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "about",
          "content": "I’m a research-minded data scientist. My current research interests focus on time series understanding and their approach with techniques expanding from fundamental statistics to machine learning. More broadly, I’m keen on probability theory, linear algebra, and machine learning. I love approaching problems in a rigorously scientific manner, finding beautiful solutions and explaining them well. This blog is my workbook, it’s been continuously updated since December 2021 so as to keep a track of my research progress. .",
          "url": "https://antounes.github.io/science/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  
  

  
  

  
  

  
  

  
      ,"page8": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://antounes.github.io/science/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}